# what is thread-safe in .net?
In .NET, a thread-safe operation or code is one that can be safely accessed and executed by multiple threads at the same time, without causing any unexpected or erroneous behavior. In other words, the result of the operation or code is predictable, consistent, and correct regardless of how many threads are accessing it concurrently.

Some examples of thread-safe constructs in .NET are:

`Immutable objects`: Objects whose state cannot be modified after creation are thread-safe by nature, as there is no chance of any thread modifying the object's state.

`Locking constructs`: `lock, Monitor, ReaderWriterLockSlim, and SemaphoreSlim` are some of the locking constructs in .NET that ensure thread-safety by providing mutual exclusion between threads.

`Concurrent collections`: The System.Collections.Concurrent namespace provides a set of thread-safe collections that can be used to safely share data between multiple threads.

`Thread-safe static methods`: Static methods that do not modify any shared state and only operate on their input parameters are inherently thread-safe.

`Pure functions`: Functions that do not modify any external state and only operate on their input parameters are inherently thread-safe.

These are just a few examples, and there are many more ways to achieve thread-safety in .NET. However, it's important to note that ensuring thread-safety is not a one-size-fits-all solution and depends on the specific scenario and requirements of the application.
# Locking and Thread Safety

A program or method is `thread-safe` if it can work correctly in any multithreading scenario. Thread safety is achieved primarily with locking and by reducing the possibilities for thread interaction.

General-purpose types are rarely thread-safe in their entirety, for the following reasons:
• The `development burden` in full thread safety can be significant, particularly if a type has many fields (each field is a potential for interaction in an arbitrarily multithreaded context).
• `Thread safety `can entail a performance cost (payable, in part, whether or not the type is actually used by multiple threads).
• A `thread-safe type` does not necessarily make the program using it thread-safe, and often the work involved in the latter makes the former redundant.

A few ways to “cheat” and have large and complex classes run safely in a multithreaded environment.
- One is to s`acrifice granularity by wrapping large sections of code—even access to an entire object—within a single exclusive lock`, enforcing serialized access at a high level.The trick is simply to use the same exclusive lock to protect access to all properties, methods, and fields on the thread-unsafe object. The solution works well if the object’s methods all execute quickly (otherwise, there will be a lot of blocking).

```c#
public class ThreadUnsafeClass
{
    private readonly object lockObj = new object();
    private int value1;
    private string value2;

    public void SetValue(int val1, string val2)
    {
        lock(lockObj)
        {
            value1 = val1;
            value2 = val2;
        }
    }

    public int GetValue1()
    {
        lock(lockObj)
        {
            return value1;
        }
    }

    public string GetValue2()
    {
        lock(lockObj)
        {
            return value2;
        }
    }
}
```
All methods that access the ThreadUnsafeClass object are protected by the same lock. This approach sacrifices granularity by protecting all properties and methods of the class with a single lock, but it ensures serialized access and avoids race conditions.

- Another way to cheat is to `minimize thread interaction by minimizing shared data`. This is an excellent approach and is used implicitly in “stateless” middle-tier appli‐ cation and web-page servers. Because multiple client requests can arrive simultane‐ ously, the server methods they call must be thread-safe. A stateless design (popular for reasons of scalability) intrinsically limits the possibility of interaction because classes do not save data between requests. Thread interaction is then limited just to the static fields that you might choose to create, for such purposes as caching commonly used data in memory and in providing infrastructure services such as authentication and auditing.

```c#
public static class StatelessServer
{
    public static int Add(int a, int b)
    {
        return a + b;
    }

    public static string Concatenate(string s1, string s2)
    {
        return s1 + s2;
    }
}

```

# Thread Safety and .NET Types

You can use locking to convert thread-unsafe code into thread-safe code. A good application of this is .NET: `nearly all of its nonprimitive types are not thread-safe `(for anything more than read-only access) when instantiated, and yet you can use them in multithreaded code if all access to any given object is protected via a lock. Here’s an example in which two threads simultaneously add an item to the same List collection and then enumerate the list:

```c#
List<string> _list = new List<string>();
new Thread(AddItem).Start();
new Thread(AddItem).Start();

void AddItem()
{
    lock (_list) _list.Add("Item " + _list.Count);
    string[] items;
    lock (_list) items = _list.ToArray();
    foreach (string s in items) Console.WriteLine(s);
}
```

In this case, we’re locking on the `_list` object itself. If we had two interrelated lists, we would need to choose a common object upon which to lock (we could nominate one of the lists, or better: use an independent field).
`Enumerating .NET collections is also thread-unsafe in the sense that an exception is thrown if the list is modified during enumeration`. `Rather than locking for the duration of enumeration, in this example, we first copy the items to an array.` This avoids holding the lock excessively if what we’re doing during enumeration is potentially time-consuming. (Another solution is to use a reader/writer lock; see “Reader/Writer Locks” on page 882.)

## Why copy items to array avoid this issue ennumeration?
Copying items to an array before enumeration can ensure thread safety because `it avoids modification of the original collection while being enumerated`. When an item is added or removed from a collection while it's being enumerated, it can cause unexpected behavior, such as an exception being thrown or some items being skipped or repeated.

`Copying the items to an array before enumeration ensures that the original collection is not being modified while being enumerated`, so we can safely iterate over the copy without the risk of unexpected behavior. Additionally, it allows us to release the lock on the original collection more quickly, which can improve performance and reduce contention in a multi-threaded environment.

# Locking around thread-safe objects
Sometimes, you also need to lock around accessing thread-safe objects. To illustrate, imagine that .NET’s List class was, indeed, thread-safe, and we want to add an item to a list:

```c#
 if (!_list.Contains (newItem)) _list.Add (newItem);
```
Regardless of whether the list was thread-safe, this statement is certainly not! The whole if statement would need to be wrapped in a lock to prevent preemption in between testing for containership and adding the new item. This same lock would then need to be used everywhere we modified that list. For instance, the following statement would also need to be wrapped in the identical lock to ensure that it did not preempt the former statement:
```c#
    _list.Clear();
```
In other words, we would need to lock exactly as with our thread-unsafe collection classes (making the List class’s hypothetical thread safety redundant).

`Note`: Locking around accessing a collection can cause exces‐ sive blocking in highly concurrent environments. To this end, .NET provides a thread-safe queue, stack, and dictionary,

# Read-only thread safety
Making types thread-safe for concurrent read-only access (where possible) is advan‐ tageous because it means that consumers can avoid excessive locking. Many .NET types follow this principle: collections, for instance, are thread-safe for concurrent readers.
Following this principle yourself is simple: if you document a type as being thread- safe for concurrent read-only access, don’t write to fields within methods that a consumer would expect to be read-only (or lock around doing so). For instance, in implementing a` ToArray()` method in a collection, you might begin by compacting the collection’s internal structure. However, this would make it thread-unsafe for consumers that expected this to be read-only.
`Read-only thread safety `is one of the reasons that enumerators are separate from “enumerables”: two threads can simultaneously enumerate over a collection because each gets a separate enumerator object.

In the absence of documentation, it pays to be cautious in assuming whether a method is read-only in nature. A good example is the Random class: when you call Random.Next(), its internal implementation requires that it update private seed values. Therefore, you must either lock around using the Ran dom class or maintain a separate instance per thread.

# Thread Safety in Application Servers
Application servers need to be multithreaded to handle simultaneous client requests. ASP.NET Core and Web API applications are implicitly multithreaded. This means that when writing code on the server side, you must consider thread safety if there’s any possibility of interaction among the threads processing client requests. Fortunately, such a possibility is rare; a typical server class is either state‐ less (no fields) or has an activation model that creates a separate object instance for each client or each request. Interaction usually arises only through static fields, sometimes used for caching in memory parts of a database to improve performance.

For example, suppose that you have a RetrieveUser method that queries a database:
```c#
    // User is a custom class with fields for user data
    internal User RetrieveUser (int id) { ... }
```

If this method were called frequently, you could improve performance by caching the results in a static Dictionary. Here’s a conceptually simple solution that takes thread safety into account:
```c#
  static class UserCache
    {
      static Dictionary <int, User> _users = new Dictionary <int, User>();
      internal static User GetUser (int id)
      {
User u = null;
        lock (_users)
          if (_users.TryGetValue (id, out u))
return u;
    u = RetrieveUser (id);
    lock (_users) _users [id] = u;
    return u;
} }
// Method to retrieve from database;
```
We must, at a minimum, lock around reading and updating the dictionary to ensure thread safety. In this example, we choose a practical compromise between simplicity and performance in locking. Our design creates a small potential for inefficiency: if two threads simultaneously called this method with the same previously unretrieved id, the RetrieveUser method would be called twice—and the dictionary would be updated unnecessarily. Locking once across the whole method would prevent this, but it would create a worse inefficiency: the entire cache would be locked up for the duration of calling RetrieveUser, during which time other threads would be blocked in retrieving any user.

For an ideal solution, we need to use the strategy we described in “Completing synchronously” on page 655. Instead of caching User, we cache Task<User>, which the caller then awaits:

```c#
static class UserCache
    {
      static Dictionary <int, Task<User>> _userTasks = new Dictionary <int, Task<User>>();
      internal static Task<User> GetUserAsync (int id) {
        lock (_userTasks)
          if (_userTasks.TryGetValue (id, out var userTask))
            return userTask;
          else
            return _userTasks [id] = Task.Run (() => RetrieveUser (id)); }
}

```
Notice that we now have a single lock that covers the entire method’s logic. We can do this without hurting concurrency because all we’re doing inside the lock is accessing the dictionary and (potentially) initiating an asynchronous operation (by calling Task.Run). Should two threads call this method at the same time with the same ID, they’ll both end up awaiting the same task, which is exactly the outcome we want.