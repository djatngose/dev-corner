# Signaling with Event Wait Handles
The simplest kind of signaling constructs are called event wait handles (unrelated to C# events). Event wait handles come in three flavors: AutoResetEvent, Manual ResetEvent(Slim), and CountdownEvent. The former two are based on the common EventWaitHandle class from which they derive all their functionality.
# AutoResetEvent
An AutoResetEvent is like a ticket turnstile: inserting a ticket lets exactly one person through. The “auto” in the class’s name refers to the fact that an open turnstile automatically closes or “resets” after someone steps through. A thread waits, or blocks, at the turnstile by calling WaitOne (wait at this “one” turnstile until it opens), and a ticket is inserted by calling the Set method. If a number of threads call WaitOne, a queue2 builds up behind the turnstile. A ticket can come from any thread; in other words, any (unblocked) thread with access to the AutoResetEvent object can call Set on it to release one blocked thread.
You can create an AutoResetEvent in two ways. The first is via its constructor: var auto = new AutoResetEvent (false);
(Passing true into the constructor is equivalent to immediately calling Set upon it.) The second way to create an AutoResetEvent is as follows:
    var auto = new EventWaitHandle (false, EventResetMode.AutoReset);

In the following example, a thread is started whose job is simply to wait until signaled by another thread (see Figure 21-1):
    class BasicWaitHandle
    {
      static EventWaitHandle _waitHandle = new AutoResetEvent (false);
  static void Main()
  {
    new Thread (Waiter).Start();
    Thread.Sleep (1000);
    _waitHandle.Set();
  static void Waiter()
  {
Console.WriteLine ("Waiting..."); _waitHandle.WaitOne(); Console.WriteLine ("Notified");
} }
// Output:
Waiting... (pause) Notified.

If Set is called when no thread is waiting, the handle stays open for as long as it takes until some thread calls WaitOne. This behavior helps prevent a race between a thread heading for the turnstile and a thread inserting a ticket (“Oops, inserted the ticket a microsecond too soon; now you’ll have to wait indefinitely!”). However, calling Set repeatedly on a turnstile at which no one is waiting doesn’t allow an entire party through when they arrive: only the next single person is let through, and the extra tickets are “wasted.”

# Disposing Wait Handles
After you’ve finished with a wait handle, you can call its Close method to release the OS resource. Alternatively, you can simply drop all references to the wait handle and allow the garbage collector to do the job for you sometime later (wait handles implement the disposal pattern whereby the finalizer calls Close). This is one of the few scenarios for which relying on this backup is (arguably) acceptable, because wait handles have a light OS burden.
Wait handles are released automatically when a process exits.

# Two-way signaling
Suppose that we want the main thread to signal a worker thread three times in a row. If the main thread simply calls Set on a wait handle several times in rapid succession, the second or third signal can become lost because the worker might take time to process each signal.
The solution is for the main thread to wait until the worker’s ready before signaling it. We can do this by using another AutoResetEvent, as follows:
    class TwoWaySignaling
    {
static EventWaitHandle _ready = new AutoResetEvent (false); static EventWaitHandle _go = new AutoResetEvent (false); static readonly object _locker = new object();
static string _message;
 static void Main() {
new Thread (Work).Start();
  _ready.WaitOne();
  lock (_locker) _message = "ooo";
_go.Set();
  _ready.WaitOne();
// First wait until worker is ready
// Tell worker to go
lock (_locker) _message = "ahhh";  // Give the worker another message
_go.Set();
_ready.WaitOne();

        lock (_locker) _message = null;
_go.Set();
}
      static void Work()
      {
        while (true)
        {
_ready.Set(); _go.WaitOne(); lock (_locker) {
            if (_message == null) return;
            Console.WriteLine (_message);
          }
} }
}
    // Output:
    ooo
    ahhh

# ManualResetEvent
As we described in Chapter 14, a ManualResetEvent functions like a simple gate. Calling Set opens the gate, allowing any number of threads calling WaitOne to be let through. Calling Reset closes the gate. Threads that call WaitOne on a closed gate will block; when the gate is next opened, they will be released all at once. Apart from these differences, a ManualResetEvent functions like an AutoResetEvent.
As with AutoResetEvent, you can construct a ManualResetEvent in two ways: var manual1 = new ManualResetEvent (false);
    var manual2 = new EventWaitHandle (false, EventResetMode.ManualReset);

    There’s another version of ManualResetEvent called ManualRe setEventSlim. The latter is optimized for short waiting times —with the ability to opt into spinning for a set number of iter‐ ations. It also has a more efficient managed implementation and allows a Wait to be canceled via a CancellationToken. ManualResetEventSlim doesn’t subclass WaitHandle; however, it exposes a WaitHandle property that returns a WaitHandle- based object when called (with the performance profile of a traditional wait handle).
 
# Signaling Constructs and Performance
Waiting or signaling an AutoResetEvent or ManualResetEvent takes about one microsecond (assuming no blocking).
ManualResetEventSlim and CountdownEvent can be up to 50 times faster in short- wait scenarios because of their nonreliance on the OS and judicious use of spinning constructs. In most scenarios, however, the overhead of the signaling classes them‐ selves doesn’t create a bottleneck; thus, it is rarely a consideration.

# CountdownEvent
CountdownEvent lets you wait on more than one thread. The class has an efficient, fully managed implementation. To use the class, instantiate it with the number of threads, or “counts,” that you want to wait on:
var countdown = new CountdownEvent (3); // Initialize with "count" of 3. Calling Signal decrements the “count”; calling Wait blocks until the count goes
down to zero:
    new Thread (SaySomething).Start ("I am thread 1");
    new Thread (SaySomething).Start ("I am thread 2");
    new Thread (SaySomething).Start ("I am thread 3");
    countdown.Wait();   // Blocks until Signal has been called 3 times
    Console.WriteLine ("All threads have finished speaking!");
    void SaySomething (object thing)
    {
Thread.Sleep (1000); Console.WriteLine (thing); countdown.Signal();
}

You can sometimes more easily solve problems for which CountdownEvent is effective by using the structured parallelism constructs that we describe in Chapter 22 (PLINQ and the Parallel class).
You can reincrement a CountdownEvent’s count by calling AddCount. However, if it has already reached zero, this throws an exception: you can’t “unsignal” a Count downEvent by calling AddCount. To prevent the possibility of an exception being thrown, you can instead call TryAddCount, which returns false if the countdown is zero.
To unsignal a countdown event, call Reset: this both unsignals the construct and resets its count to the original value.
Like ManualResetEventSlim, CountdownEvent exposes a WaitHandle property for scenarios in which some other class or method expects an object based on WaitHandle.


# Creating a Cross-Process EventWaitHandle
EventWaitHandle’s constructor allows a “named” EventWaitHandle to be created, capable of operating across multiple processes. The name is simply a string, and it can be any value that doesn’t unintentionally conflict with someone else’s! If the name is already in use on the computer, you get a reference to the same underlying EventWaitHandle; otherwise, the OS creates a new one. Here’s an example:
    EventWaitHandle wh = new EventWaitHandle (false, EventResetMode.AutoReset,
                                          @"Global\MyCompany.MyApp.SomeName");
If two applications each ran this code, they would be able to signal each other: the wait handle would work across all threads in both processes.
Named event wait handles are available only on Windows.

# Wait Handles and Continuations
Rather than waiting on a wait handle (and blocking your thread), you can attach a “continuation” to it by calling ThreadPool.RegisterWaitForSingleObject. This method accepts a delegate that is executed when a wait handle is signaled:
var starter = new ManualResetEvent (false);
    RegisteredWaitHandle reg = ThreadPool.RegisterWaitForSingleObject
     (starter, Go, "Some Data", -1, true);
Thread.Sleep (5000);
Console.WriteLine ("Signaling worker..."); starter.Set();
Console.ReadLine();
reg.Unregister (starter); // Clean up when we’re done.
void Go (object data, bool timedOut)

    {
      Console.WriteLine ("Started - " + data);
      // Perform task...
}
    // Output:
    (5 second delay)
    Signaling worker...
    Started - Some Data
When the wait handle is signaled (or a timeout elapses), the delegate runs on a pooled thread. You are then supposed to call Unregister to release the unmanaged handle to the callback.
In addition to the wait handle and delegate, RegisterWaitForSingleObject accepts a “black box” object that it passes to your delegate method (rather like Parameteri zedThreadStart) as well as a timeout in milliseconds (-1 meaning no timeout) and a Boolean flag indicating whether the request is a one-off rather than recurring.
You can reliably call RegisterWaitForSingleObject only once per wait handle. Calling this method again on the same wait handle causes an intermittent failure, whereby an unsignaled wait handle fires a callback as though it were signaled.
This limitation makes (the nonslim) wait handles poorly suited to asynchronous programming.

# WaitAny, WaitAll, and SignalAndWait
In addition to the Set, WaitOne, and Reset methods, there are static methods on the WaitHandle class to crack more complex synchronization nuts. The WaitAny, WaitAll, and SignalAndWait methods perform signaling and waiting operations on multiple handles. The wait handles can be of differing types (including Mutex and Semaphore given that these also derive from the abstract WaitHandle class). ManualResetEventSlim and CountdownEvent can also partake in these methods via their WaitHandle properties.
WaitAll and SignalAndWait have a weird connection to the legacy COM architecture: these methods require that the caller be in a multithreaded apartment, the model least suitable for interoperability. The main thread of a WPF or Windows Forms application, for example, is unable to inter‐ act with the clipboard in this mode. We discuss alternatives shortly.
WaitHandle.WaitAny waits for any one of an array of wait handles; Wait Handle.WaitAll waits on all of the given handles, atomically. This means that if you wait on two AutoResetEvents:
• WaitAny will never end up “latching” both events.
• WaitAll will never end up “latching” only one event.
SignalAndWait calls Set on one WaitHandle and then calls WaitOne on another WaitHandle. After signaling the first handle, it will jump to the head of the queue in waiting on the second handle; this helps it succeed (although the operation is not truly atomic). You can think of this method as “swapping” one signal for another, and use it on a pair of EventWaitHandles to set up two threads to rendezvous, or “meet,” at the same point in time. Either AutoResetEvent or ManualResetEvent will do the trick. The first thread executes the following:
    WaitHandle.SignalAndWait (wh1, wh2);
The second thread does the opposite:
    WaitHandle.SignalAndWait (wh2, wh1);

# The Barrier Class
The Barrier class in .NET provides a way to synchronize multiple threads at specific points in their execution. It is useful when you have a group of threads that need to work together in phases, and you want to ensure that all threads have completed the current phase before moving on to the next one.

A common example is parallel processing of a large data set. You might divide the data set into chunks and assign each chunk to a different thread for processing. However, if the processing of one chunk depends on the processing of another, you need to ensure that all threads have finished processing their current chunk before moving on to the next one.

Here's an example of using the Barrier class to synchronize the processing of a data set:
```c#
public void ProcessDataInParallel()
{
    int chunkSize = 100;
    int numChunks = 10;
    Barrier barrier = new Barrier(numChunks); // create a barrier with the number of phases

    for (int i = 0; i < numChunks; i++)
    {
        int chunkIndex = i;
        Task.Run(() =>
        {
            // process the current chunk
            ProcessChunk(chunkIndex, chunkSize);

            // wait for all threads to finish processing their current chunk
            barrier.SignalAndWait();
        });
    }
}

private void ProcessChunk(int chunkIndex, int chunkSize)
{
    // process the data in the current chunk
}

```

# Lazy Initialization
A frequent problem in threading is how to lazily initialize a shared field in a thread-safe fashion. The need arises when you have a field of a type that’s expensive to construct:
class Foo {
      public readonly Expensive Expensive = new Expensive();
...
    }
    class Expensive {  /* Suppose this is expensive to construct */  }
The problem with this code is that instantiating Foo incurs the performance cost of instantiating Expensive—regardless of whether the Expensive field is ever accessed. The obvious answer is to construct the instance on demand:
class Foo {
  Expensive _expensive;
  public Expensive Expensive
  {
get {
// Lazily instantiate Expensive
if (_expensive == null) _expensive = new Expensive();
          return _expensive;
        }
}
... }
The question then arises, is this thread-safe? Aside from the fact that we’re accessing _expensive outside a lock without a memory barrier, consider what would happen if two threads accessed this property at once. They could both satisfy the if state‐ ment’s predicate and each thread end up with a different instance of Expensive. Because this can lead to subtle errors, we would say, in general, that this code is not thread-safe.
The solution to the problem is to lock around checking and initializing the object:
Expensive _expensive;
readonly object _expenseLock = new object();
    public Expensive Expensive
    {
get {
        lock (_expenseLock)
{

  if (_expensive == null) _expensive = new Expensive();
          return _expensive;
        }
} }
Lazy<T>
The Lazy<T> class is available to help with lazy initialization. If instantiated with an argument of true, it implements the thread-safe initialization pattern just described.
Lazy<T> actually implements a micro-optimized version of this pattern, called double-checked locking. Double-checked locking performs an additional volatile read to avoid the cost of obtaining a lock if the object is already initialized.
To use Lazy<T>, instantiate the class with a value factory delegate that tells it how to initialize a new value, and the argument true. Then, access its value via the Value property:
    Lazy<Expensive> _expensive = new Lazy<Expensive>
      (() => new Expensive(), true);
public Expensive Expensive { get { return _expensive.Value; } }
If you pass false into Lazy<T>’s constructor, it implements the thread-unsafe lazy initialization pattern that we described at the beginning of this section—this makes sense when you want to use Lazy<T> in a single-threaded context.