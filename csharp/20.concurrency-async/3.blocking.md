# BLocking
`A thread can be blocked` when it needs to pause its execution for some reason, such as waiting for some resource to become available, waiting for a lock to be released, waiting for input or output operations to complete, or waiting for another thread to finish its task.

When a thread is blocked, it gives up its processor time slice, allowing other threads to execute on the CPU. This helps to ensure that CPU resources are used efficiently and that multiple threads can make progress concurrently.

Some common scenarios where a thread might be blocked include:

`Waiting for a lock`: When a thread needs to access a shared resource that is currently locked by another thread, it must wait until the lock is released before it can proceed. This is a common scenario in multithreaded programs that use locks to synchronize access to shared data structures.

`Waiting for I/O`: When a thread needs to read or write data from a file or network socket, it may be blocked until the operation completes. This is because I/O operations typically involve waiting for data to be transferred over the network or written to disk, and during this time the CPU can be used for other tasks.

`Sleeping`: A thread may be blocked when it calls the Sleep function or a similar function that causes it to pause for a specified amount of time. During this time, the thread consumes no processor time and allows other threads to execute.

`Waiting for another thread to finish`: When one thread needs the results of another thread's work, it may wait for that thread to finish using the Join method. During this time, the waiting thread is blocked and consumes no processor time.

In summary, a thread is blocked when it needs to pause its execution for some reason, and during this time it consumes no processor time until its blocking condition is satisfied.

`ThreadState` is a flags enum, combining three “layers” of data in a bitwise fashion. Most values, however, are redun‐ dant, unused, or deprecated. The following extension method strips a ThreadState to one of four useful values: Unstarted, Running, WaitSleepJoin, and Stopped:
public static ThreadState Simplify (this ThreadState ts) {
return ts & (ThreadState.Unstarted | ThreadState.WaitSleepJoin |
ThreadState.Stopped);
}
The ThreadState property is useful for diagnostic purposes but unsuitable for synchronization, because a thread’s state can change in between testing ThreadState and acting on that information.

`When a thread blocks or unblocks, the OS performs a context switch. This incurs a small overhead, typically one or two microseconds.`

# when a thread is blocked, is it risk?
When a thread is blocked, it is not necessarily a risk in and of itself. It is a normal part of thread execution and can occur for many reasons, such as waiting for a lock to be released, waiting for a network operation to complete, or waiting for user input.

However, if blocking occurs excessively or for long periods of time, it can lead to performance problems or even deadlocks in certain situations. A deadlock occurs when two or more threads are blocked waiting for each other to release a lock or a resource, which can result in a system hang or crash.

Therefore, it is important to carefully design your multi-threaded code to avoid unnecessary blocking and minimize the risk of deadlocks. This can involve techniques such as using non-blocking algorithms, minimizing lock contention, and avoiding long-running blocking operations.

In summary, while thread blocking itself is not necessarily a risk, it can become a problem if it occurs excessively or leads to deadlocks. It is important to carefully consider the design of your multi-threaded code to minimize the risk of these issues.


# I/O-bound versus compute-bound
An operation that spends most of its time waiting for something to happen is called `I/O-bound`—an example is `downloading a web page` or calling `Console.ReadLine`. (`I/O-bound` operations typically involve input or output, but this is not a hard requirement: `Thread.Sleep `is also deemed I/O-bound.) In contrast, an operation that spends most of its time performing CPU-intensive work is called compute- bound.

# Blocking versus spinning
An I/O-bound operation works in one of two ways: it either waits synchronously on the current thread until the operation is complete (such as `Console.ReadLine, Thread.Sleep, or Thread.Join`), or it operates asynchronously, firing a callback when the operation finishes in the future
I/O-bound operations that wait synchronously spend most of their time blocking a thread. They can also “spin” in a loop periodically:
    while (DateTime.Now < nextStartTime)
      Thread.Sleep (100);

Blocking and spinning are two different techniques that can be used to manage threads and their execution.

Blocking occurs when a thread is paused, waiting for a certain condition to be met, such as waiting for a lock to be released, waiting for user input, or waiting for a network operation to complete. While the thread is blocked, it does not consume CPU time and other threads can continue executing. Blocking is a common technique in multi-threaded programming and is used to prevent race conditions and ensure thread safety.

On the other hand, `spinning`, also known as busy waiting, occurs when a thread repeatedly checks a certain condition in a loop, without yielding the processor to other threads or processes. While spinning, the thread consumes CPU time, even if it is not actually performing useful work. Spinning is typically used when waiting for short periods of time or when there are no other threads or processes contending for the CPU.For example, spinning can be used when waiting for a lock to be released in a multi-threaded application. In this case, the thread can repeatedly check whether the lock is available, and then acquire the lock when it becomes available.



In general, `blocking` is preferred over spinning because it allows the CPU to be used more efficiently by other threads and processes. Spinning can be wasteful of CPU resources and can lead to performance issues, particularly if there are many threads busy waiting at the same time or if the wait time is long. However, spinning can be useful in some situations, such as when waiting for short periods of time or when there are no other threads or processes contending for the CPU.

There are a couple of nuances with regard to spinning versus blocking. First, `spinning very briefly can be effective when you expect a condition to be satisfied soon (perhaps within a few microseconds) because it avoids the overhead and latency of a context switch`. .NET provides special methods and classes to assist—see the online supplement “SpinLock and SpinWait”.
Second, blocking does not incur a zero cost. This is because `each thread ties up around 1 MB of memory for as long as it lives and causes an ongoing administrative overhead for the CLR and OS`. For this reason, blocking can be trou‐ blesome in the context of heavily I/O-bound programs that need to handle hundreds or thousands of concurrent opera‐ tions. Instead, such programs need to use a callback-based approach, rescinding their thread entirely while waiting. This is (in part) the purpose of the asynchronous patterns that we discuss later.

# Local Versus Shared State
The CLR assigns each thread its own memory stack so that local variables are kept separate. In the next example, we define a method with a local variable and then call the method simultaneously on the main thread and a newly created thread:
    new Thread (Go).Start();      // Call Go() on a new thread
    Go();                         // Call Go() on the main thread
void Go() {
      // Declare and use a local variable - 'cycles'
      for (int cycles = 0; cycles < 5; cycles++) Console.Write ('?');
    }

A separate copy of the cycles variable is created on each thread’s memory stack, and so the output is, predictably, 10 question marks.
Threads share data if they have a common reference to the same object or variable:
    bool _done = false;
    new Thread (Go).Start();
    Go();
void Go() {
if (!_done) { _done = true; Console.WriteLine ("Done"); } }

Both threads share the _done variable, so “Done” is printed once instead of twice. Local variables captured by a lambda expression can also be shared:
    bool done = false;
    ThreadStart action = ()

    if (!done) { done = true; Console.WriteLine ("Done"); }
    };
    new Thread (action).Start();
    action();
More commonly, though, fields are used to share data between threads. In the following example, both threads call Go() on the same ThreadTest instance, so they share the same _done field:
    var tt = new ThreadTest();
    new Thread (tt.Go).Start();
    tt.Go();
    class ThreadTest
    {
bool _done;
      public void Go()
      {
        if (!_done) { _done = true; Console.WriteLine ("Done"); }
      }
}
Static fields offer another way to share data between threads:
class ThreadTest
{
  static bool _done;
  static void Main()
  {
// Static fields are shared between all threads
// in the same process.
new Thread (Go).Start();
Go(); }
      static void Go()
      {
        if (!_done) { _done = true; Console.WriteLine ("Done"); }
      }
}
All four examples illustrate another key concept: that of thread safety (or rather, lack of it!). The output is actually indeterminate: it’s possible (though unlikely) that “Done” could be printed twice. If, `however, we swap the order of statements in the Go method, the odds of “Done” being printed twice go up dramatically`:
```c#
static void Go() {
      if (!_done) { Console.WriteLine ("Done"); _done = true; }
    }
```
The problem is that one thread can be evaluating the if statement at exactly the same time as the other thread is executing the WriteLine statement—before it’s had a chance to set done to true.