
# PFX Components
PFX comprises two layers of functionality, as shown in Figure 22-1. The higher layer consists of two structured data parallelism APIs:` PLINQ and the Parallel class`. The lower layer contains the task parallelism classes—plus a set of additional constructs to help with parallel programming activities.
  - `Task Parallel Library`: Parallel class, Task parallelism, Concurrent collection, Spinning privimites, Slim sinagling constructs, Lazy init types
  - `Structured data parallelism`: Parallel class, PLINQ
  - CLR thread pool
  - Threads

## PLINQ
`PLINQ` offers the richest functionality:
  - it automates all the steps of parallelization— including partitioning the work into tasks
  -  executing those tasks on threads, and collating the results into a single output sequence. It’s called declarative—because you simply declare that you want to parallelize your work (which you structure as a LINQ query) and let the runtime take care of the implementation details. In contrast, the other approaches are imperative, in that you need to explicitly write code to partition or collate. As the following synopsis shows, in the case of the Parallel class, you must collate results yourself; with the task parallelism constructs, you must partition the work yourself, too:

```
                    paritions work          Collates results
PLINQ                   Yes                       Yes
Parallel class          Yes                       No
PFX's task parallelism  No                        No
```

## concurrent collections and spinning primitives
The concurrent collections and spinning primitives help you with lower-level paral‐ lel programming activities. These are important because PFX has been designed to work not only with today’s hardware but also with future generations of processors with far more cores. If you want to move a pile of chopped wood and you have 32 workers to do the job, the biggest challenge is moving the wood without the work‐ ers getting in one another’s way. It’s the same with dividing an algorithm among 32 cores: if ordinary locks are used to protect common resources, the resultant blocking can mean that only a fraction of those cores are ever actually busy at once. The concurrent collections are tuned specifically for highly concurrent access, with the focus on minimizing or eliminating blocking. PLINQ and the Parallel class themselves rely on the concurrent collections and on spinning primitives for efficient management of work.

# When to Use PFX
The primary use case for `PFX` is parallel programming: `leveraging multicore processors to speed up computationally intensive code`
A challenge in parallel programming is Amdahl’s law, which states that the maxi‐ mum performance improvement from parallelization is governed by the portion of the code that must execute sequentially. For instance, if only two-thirds of an algorithm’s execution time is parallelizable, you can never exceed a threefold performance gain—even with an infinite number of cores.

`Other Uses for PFX`
The parallel programming constructs are useful not only for leveraging multicores but in other scenarios as well:
• The `concurrent collections` are sometimes appropriate when you want a thread-safe queue, stack, or dictionary.
• `BlockingCollection` provides an easy means to implement producer/consumer structures, and is a good way to limit concurrency.
• `Tasks` are the basis of asynchronous programming, as we saw in Chapter 14.

So, `before proceeding`, it’s worth verifying that the bottleneck is in parallelizable code.
  - It’s also worth considering whether your code needs to be computationally intensive—optimization is often the easiest and most effective approach.
  -  There’s a trade-off, though, in that some optimization techniques can make it more difficult to parallelize code.

The easiest gains come with what’s called embarrassingly parallel problems—this is when a job can be easily divided into tasks that efficiently execute on their own (structured parallelism is very well suited to such problems). Examples include many image-processing tasks, ray tracing, and brute-force approaches in mathe‐ matics or cryptography. An example of a non-embarrassingly parallel problem is implementing an optimized version of the quicksort algorithm—a good result takes some thought and might require unstructured parallelism.