# Nonexclusive locking
The nonexclusive locking constructs serve to limit concurrency. In this section, we cover `semaphores` and `read/writer locks`, and also illustrate how the SemaphoreSlim class can limit concurrency with asynchronous operations.

Earlier, we said that the need for synchronization arises even in the simple case of assigning or incrementing a field. Although locking can always satisfy this need, a contended lock means that a thread must block, suffering the overhead of a context switch and the latency of being descheduled, which can be undesirable in highly concurrent and performance-critical scenarios. The .NET Frameworkâ€™s nonblocking synchronization constructs can perform simple operations without ever blocking, pausing, or waiting.

Writing nonblocking or lock-free multithreaded code properly is tricky! Memory barriers, in particular, are easy to get wrong (the volatile keyword is even easier to get wrong). Think carefully whether you really need the performance benefits before dismissing ordinary locks. Remember that acquiring and releasing an uncontended lock takes as little as 20ns on a 2010-era desktop.

The nonblocking approaches also work across multiple processes. An example of where this might be useful is in reading and writing process-shared memory.

# why Memory Barriers and Volatility is non blocking  sync?

Memory barriers and volatility are considered non-blocking synchronization mechanisms because they don't explicitly block or suspend threads to enforce synchronization. Instead, they provide guarantees about the ordering and visibility of memory operations, allowing for safe and efficient concurrent execution without the need for thread suspension.

`Memory Barriers`:
Memory barriers are instructions or constructs that enforce specific ordering guarantees for memory operations. They ensure that memory accesses before the barrier are completed before the barrier itself, and memory accesses after the barrier are not reordered to occur before the barrier.

Memory barriers provide synchronization by preventing certain types of reordering or caching optimizations performed by the processor and memory subsystem. They help establish a consistent view of memory across threads, ensuring that changes made by one thread are visible to other threads in the desired order.

Memory barriers are typically lightweight operations that allow for efficient synchronization without blocking or suspending threads. They ensure that memory operations appear to occur in a predictable and consistent manner across different threads.

`Volatility`:
Volatility is a concept associated with memory access that affects the visibility of changes made by one thread to other threads. When a variable is marked as volatile, it indicates to the compiler and runtime that the variable's value may change unexpectedly, and therefore, appropriate synchronization mechanisms should be used to ensure visibility.

A volatile variable ensures that reads and writes to that variable are not optimized or reordered by the compiler or processor. It guarantees that any change made by one thread to a volatile variable is visible to other threads immediately or as soon as possible.

Volatility, like memory barriers, is a non-blocking synchronization mechanism. It ensures visibility of changes across threads without requiring thread suspension or blocking.

In summary, memory barriers and volatility are non-blocking synchronization mechanisms because they allow for concurrent execution without explicitly blocking or suspending threads. They provide guarantees about the ordering and visibility of memory operations, ensuring consistent and correct behavior in multithreaded scenarios.